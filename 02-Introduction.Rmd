# (PART) Statistical Modelling {-}

# Introduction

What advice would you give me to deliver a wide range of powerful analytics on a  tight time budget? Well, I believe the answer is to use the R Studio system. I believe this doubly as I recognise R Studio as a "Mass Customisation Technology" from my studies at Business School.

Over the next 50 pages, you will be introduced to the key components of the R Studio system and we will explore examples from Business Analytics of how the R studio is something radically new. But first lets get some extra motivation by exploring this Mass Customisation idea.

# Mass Customistion

Mass Customisation is a powerful idea taught at leading business schools across the World. I was introduced to the concept over a coffee in Munich by a Graduate Student in Logistics from the Netherlands! It runs like this. A century ago, economies of scale in Manufacturing Processes brought down the price of mass-produced goods to the extent that customers were willing to trade-off their individual preferences for a cheap but standardized mass-produced good (Hravard). Nowadays, new forms of organisation and technology are radically reducing this trade-off between individuality and cost (Slack). Thus enabling Mass Production of Customised Goods/Services, so called "Mass Customisation".

The key idea is to use customisable technology. This means each individual order is produced by technology customised to the customer's individual performance needs. In the traditional approach all orders are produced using the same technology, which is defined at the product level. As we'll see below, this massively increases the influence of performance trade-offs and meeting the customer's needs becomes a compromise.

## Operations Strategy Concept

Mass Customisation is an extreme form of Operations Strategy. A holistic approach to running a business that has a particular meaning in business schools. 

### Operations Strategy in Pictures

Operations strategy is shown schematically in figuree xx below:

In the right-hand box you see the Market Requirements. These are summarised by Performance Objectives such as Quality, Speed, Flexibility, Dependability and Cost. Performance Objectives are influenced by which customer needs the company wants to address and how they will attract these customers relative to the their competitors. 

The left hand box is called Operations Resources. These include the Assets of the firm (such as Software), the Processes of the firm (eg. ) and the Capabilities of the firm (eg. know-how of staff). 

In the centre is Operations Strategy. The process of reconciling the Resources available to the company with the Performance Objectives it has set. Its key tenet is that Business performance is higher when such a reconciliation takes place.

Mass Customisation is extreme because it treats each individual customer as a separate market whose orders are to be handled with a customised set of resources. Traditionally segmentation happens at the less granular, product level.

### Traditional Approach to product segmentation

The traditional approach to product segmentation is much less granular. 

Porter summarised three distinct approaches: The Low Cost, Differentiation and Focus strategies. Each order of the Low Cost product is produced using the same technology, irregardless of the customer need. 


figures gfgf

The key enabler of mass customisation is an ability to quickly and cheaply customise production technology to an individual order. This massively increases the ability to produce a wide variety of services at low cost

### Traditional Approach to order fulfillment

Traditionally if a customer for a Low Cost Product desires a significantly higher quality, it can only be achieved at large cost.

Figure of movement along performance curve.

In Mass Customisation, the idea is to fulfil the same order without a large change in cost.

Figure of performance curve in mass customisation






To quote: https://hbr.org/1997/01/the-four-faces-of-mass-customization

"As the concept of a mass market gained currency a century ago with the success of such giants as Sears, A&P, Coca-Cola, and Ford, all too many managers lost sight of a simple fact known for ages by every butcher, cobbler, and corner grocer: every customer is unique. Economies of scale in manufacturing and distribution brought down the price of mass-produced goods so much that all but the most well-to-do customers were often willing to forgo their individuality and settle for standardized—but very affordable—goods."


# Microsoft Analytics Tool {#Research}

Have you seen the film the "Big Short"? It is a great movie and highlights many major failings of the US sub-prime mortgage industry. The main villains are the fast living, money grabbing Investment bankers from New York. However the Credit Rating Agencies, quasi-governmental organisations manadated to defend investors from poor quality investments, were revealed as partners in crime.

Rating Agencies are weird businesses. If you imagine the the glitzy sky-scraper offices of Investment Banks then you are also correctly picturing a Rating Agency. But unlike Investment Banks which rely on the best brains to continuously generate new business, Rating Agencies do not generate any new business. They simply provide a financial health check service which is a legal requirement for many Institutional Investors.

So the story I want to guide you through goes something like this. A mortgage is a very large loan made by a lending business to individuals purchasing a home. The mortgage typically represents 3-5 times the salary of the borrower and can take up to 50 years to repay! Investors in mortgage lending businesses are clearly motivated to make sure these repayments happen. To this end, they can obtain a financial health check from a Rating Agency. What has this got to do with Microsoft Analytics tools?

One of the "Big Three Credit Ratings Agencies" a company called Fitch Ratings Inc developed a health check tool. They wanted to provide a free-to-download tool that runs on the Microsoft Excel desktop-package. Their objective was to make the tool perform the complex calculations so that the user can focus on their investment questions: can the customer afford to repay his loan and what is the resale value of the property. OK! But why do we care about this tool?

Whilst not every Microsoft analytics tool looks like what follows. I am putting this tool under the spotlight as a "best in class" Microsoft product. Why? Well, if you imagine a financial servicse company making a huge amount of cash and employing over 2000 analysts, then its models should be be pretty high quality. Right?

On its website Fitch describes its tool as:

-    The tool provides "An Analytical model that helps with the risk assessment of Residential Mortgage loans". 
-    The "sustainable home price model" is used to determine the true value of a property. A sustainable long-term house price is calculated based on six key drivers identified through regression anaysis on a data-set from 1976-2011.
-    The "Probability of Default model" uses a regression-based analysis to estimate the probability that a customer stops making payments. There are 13 independent variables in the model, 11 individual loan and borrower attributes and one related to economic risk factors. The model is based on data from 1991-2009.

When you click download, you'll get a zip file containing how-to documentation and a Microsoft Excel file containing the tool. 

Clearly when dealing with business users, achieving user acceptance is as important as model sophistication. You can see in figure xx that Fitch achieved this: 

(ref:ResiEMEA1) A screenshot of what the downloaded .zip file contains. Figure a.) Shows the individual files including user guides but also two excel tools. b.) The Datatape tool c.) The data template and d.) The Model tool

```{r,echo=FALSE, fig.cap='(ref:ResiEMEA1)'}
knitr::include_graphics("Analysis/Images/ResiEMEA1.png")
```


For business users with limited I.T. credentials, an Excel desktop tool requires installing no new software nor specialist training. It also contains no external data links so there are no issues regarding data security. Whilst the download does contain easy to read documentation, the tool guides the user through the analysis in a self-explanatory way using a pop-up "wizard" application.

As with all calculation engines, Fitch's tool requires data in a specific format. If you look at the figures xx xx, you will see a number of data validation and certification steps. 

(ref:ResiEMEA2) Screenshots of the wizard guiding the user through four data validation steps

```{r,echo=FALSE, fig.cap='(ref:ResiEMEA2)'}
knitr::include_graphics("Analysis/Images/ResiEMEA2.png")
```


This is actually the reason why a pop-up wizard was required.Unfortunately there is no inbuilt wizard within Microsoft Desktop products and an enhancement is required to create one. This requires the support of a software development.

I mentioned above that there are two key models in the tool. The "sustainable home price model" calculates property resale values. Whilst this was sold to us as a "regression model" using six factors, it ends up being an accounting based calculation with just two adjustments! What has gone wrong?

(ref:ResiEMEA3) Screenshot of the resale value calculation

```{r,echo=FALSE, fig.cap='(ref:ResiEMEA3)'}
knitr::include_graphics("Analysis/Images/ResiEMEA3.png")
```


I think the critical flaw in this tool is a reliance on software developers to build the Wizard. For sure the professional looking and robust tool is a credit to the quality of the software team. Business users would never have been able to specify something so clean, nor would the analytics team ever have had the skills to build it! 

However building successful analytics tools takes many iterations and much time. You will never get correctly specified requirements from business users until they have seen the first product release! Software developers are precious resources which are either no longer available or too expensive for these later iterations. So all you ever get is a first release and in this case the calculation engine was highly simplified!  

Lets re-cap. We started with technical documentation describing a regression based calculation engine. This engine was wrapped up in a desktop tool "Microsoft Excel" for user acceptance purposes. A data validation process is a key preliminary step to running the model and a custom wizard guides users through this process. This wizard would have been built by a software development team. Finally we reviewed the "home price" calcualtion component and found that it was much less sophiticated than we were expecting. A reliance on specialised software developers was identified as a cause of this simplification.




## Research Methodology

My goal was to use a multiple linear regression model to accurately predict house prices. The starting point of my research was a sample data set from the website [kaggle](). The data contains property sales prices for transactions between May 2014 and May 2015 in King County USA which includes Seattle.

I started my research project by reviewing online literature on house price prediction. This "meta analysis" had two purposes:

- It helped select variables to include in the model 
- It added credibility to my findings if I incorporated and built on the empirical evidence of previous research studies

I performed significant data-enrichment activities. This involved API calls to the online repositories [Google]() and [Zillow](). My goal was to append to the data-set certain key variables identified in the "meta analysis".

I performed several formal model selection procedures. This involved:

- Variable selection
- Model fitting
- Diagnostics
- Selection between competing models

I evaluated model performance on a "hold-out sample" representing 20% of the original data-set. Model performance was evaluated through evaluating "out of sample" prediction accuracy.

## Meta Analysis/Literature Review

My data set from [Kaggle]() only included information on **micro variables**. These *micro variables* related to the intrinsic features of a property and its immediate environment. By contrast, *macro variables* were excluded. These relate to the "external environment" of the property (see examples below). 

By excluding macro variables, the parameter estimates of my model could be **biased**. For example, the data comes from the period from May 2014 to May 2015 when the external environment in the US was relatively stable. This means that the level of noise in the data could be artificially low and the explanatory power of micro variables overstated.

To mitigate this risk I performed a quick online literature search and identified micro variables which previous research studies have found to be significant predictors of property price. I have included these variables as the starting point of the model selection process in Section \@ref(Classical). This should lead as much overlap as possible between my model and previous studies and stops me relying 100% on a statistical model selection procedure which is vulnerable to bias.

In Section \@ref(CompSci), I take a more pro-active approach to dealing with potential bias. I use the results of a data-enrichment exercise to extend my original data-set to include **macro variables**. Furthermore these macro variables are exactly those which previous studies found to be significant predictors of property prices. 

#### Micro variable Examples

- Building features
- Interior Layout and Size
- Quality of fixture and fittings

#### Macro variable Examples

- Nearby amenities
- Cost of mortgage borrowing in the area
- unemployment levels in the job market
- demographics 

#### Results of online search

The table below shoes the micro-variables identified online, the reference article and the matching field in the database. Article 1 is [@galati2011macro], Article 2 is [@candas2015], Website 1 is [@rightmove]


|Micro-variable         | Reference | Data-Set Field  |
|-----------------------|:---------:|-----------------|
|Year of Construction   | Article 1 |Construction Year|
|Size of Living Room    | Article 1 |Living Space     |
|Presence of Garage     | Article 1 |                 |
|Presence of Garden     | Article 1 |                 |
|Type of House          | Article 1 |Number of Floors |
|Large City vs not large| Article 1 |Seattle ZipCode  |
|Degree of Urbanization | Article 1 |                 |
|Floor No               | Article 2 |Floors           |
|Heating System         | Article 2 |Renovation Year  |
|Earthquake Zone        | Article 2 |                 |
|Rental Value           | Article 2 |                 |
|Land Value             | Article 2 |                 |
|Parcel Area            | Article 2 |Total Area       |
|Zoning                 | Article 2 |Zipcode          |
|Proximity to Amenities | Website 1 |                 |
|Number of Bedrooms     | Website 1 |Bedrooms         |
|Number of Bathrooms    | Website 1 |Bathrooms        |
|Age of Building        | Website 1 |                 |
|Condition of Interior  | Website 1 |Condition/Grade  |

